# Simple Storage Service (S3)

object-based storage service, serverless storage in the cloud, dont worry about filesystems or disk space


data storage architecture manages data as objects instad as other storage architectures
file systems which manages data as a files and fire hierarchy, block storage manages data as blocks within sectors and tracks

unlimited storage

s3 object and s3 bucket

object contain data, consist of key, value, version id, and meta data can store data from 0 bytes to 5 bytes

buckets hold objects, can also have folders which in turn hold objects, names must be unique


## storage classes

standard (default) -> intelligent tiering -> standard infrequently accessed (IA) -> one zone IA -> glacier -> glacier deep archive

standard us 99.99 percent availability replicated across 3 AZs

intel used ml to analyze and determine appro storage, decides for you which storage class you use, helps you save money

stan ia cheaper if you access files less than once a month

one zone mean objects only exist in one az

glacier is for long-term cold storage

glacier deep is lowest cost, datat retrieval time is 12 hours, best for long archival data

## storage classes comparison

s3 gurantees platofrm built for 99.99 availability, amazon gurantess availability 99.9 and 11'9s of durability

## s3 security

all buckets private by default

access control configured by bucket policies and access control lists

acl legacy feature of controlling access to bucket, pretty simple

logging per request can be turned on a bucket, log files generated and saved in diff bucket or even bucket in diff aws account

bucket policies- write a json document policy to define complex rule access

## s3 encryption

traffic btwn localhost and s3 achieved via ssl/tls

server side encryption encryption at rest

amazon help encrypt data

s3 managed keys
sse aes s3 handles key uses aes 256 algo
sse kms envelope encryptions amazon kms and you manage keys
sse c customer provided key

client side encryption
you encrypt your own files b4 uploading them to s3

## data consistency

new objects puts
-read after write consistency
when s3 object upload able to read immediately after write

overwrite puts or deelte objects (deletes)
eventual consistency
when done takes time for s3 to replicate versions to azs
if read immediate, s3 may return an old copy, must waite few sec before read

## cross-region replication

object uploaded will be auto replicate to another region if enabled, provide higher durabilit and potential disaster recov for obj
must have versioning turned on in source and destination nucket

## versioning

store all versions of an object an s3
prevent data lodd, once enavled cannot be disabled, only suspend on bucket
fully itnegrate w s3 lifecycle rules
mfa delete feature provides extra protection against deletion of data

## lifecycle management

automate process moving obj to diff storage classes or delete altogether
can be used w versioning
can apply to both current and previous versions

eg after 7 days move to glacier, after 365 days permananently delete, may need for compliance

## tranfer acceleration

fast and secure files of over long distances btwn end users and s3 bucker

utilize cloudronts distrivuted edge locations

instead of uploading to bucket, usrs use distrinct url for edge location
as data arrives at edge location data auto routed to s3 over special optimized network path (aka amazons backbone entwork)

## presigned urls

generate urls which provide temp access to an object to upload or download object dara, 
commonly used to provides access to priv objects can use aws cli or asw sdk to generate predesigned urls

eg you have a web app which needs to allow users to downlod files from password protect part of webapp, web app genrates predesigned url that expire after 5 sec, user downloads file

## mfa delete

ensure users cannot delete objects from bucket unless they provide their mfa code

can only be down if aws cli is turned on todelete mfa or bucket must have versioning turned on

only bucket woner logged in as root user can delete objects from bucket

## create and delete object follow along

done this before, so look

s3 buckets are global, not regional!
doesnt mean buckets dont belong to region, it is just interface feature

bucketname must be unique and dns compliant, think of it like a web domain name

## upload files and make public

done this before, so look around

can upload images 

can see when uploaded, object url, stoage class, size, etag, serverside encryption

if you want url to work, must make bucket public, go to permissions to do this, once made public, you can share link with the world!

## versioning follow along

keep track of versions, go to properties to turn on versioning
cant turn off versioning, only suspend it!
previous version has null of version id, newest version will have id
if you want new version of data to be public, must be set as public
if delete data, you only delete the most recent version, not the older versions, older version will then get the version id
if you upload new version of data that was uploaded before versioning, new version will have nersion id, old version has null bc it was uploaded pre-versioning

## encryption follow along

go to bucket, go to properties, go to default encryption, choose btwn aes 256 or aws kms, aes 256 is easiest, does not impact existing files in bucket, youd have to go to individual files

## s3 cli

its like git cli
can see all your s3 files
can add stuff to s3 bucket, can upload/download stuff to/from s3 bucket

## lifecycle policies

want to change storage class? go to properties, can save money
go to bucket, management, lifecycle -> enter rule name and storage class transition object transition and days after creation (has to be more than 30 days), can also set expiration to completely delete files win x number of days (this will automate the process)

## cross-region replication followalong

copy one file from a bucket to another bucket in another account or region

create destination bucket, turn on versioning in both buckets, ready to trun on cross-region replication

overview? to replication, add rule

## bucket polcies foollow along

 go to permissions and go to bucket policy editor, they have a permissions generator so you dont have to write completely by scratch
 
 seelct policy type, add statement, generate policy, and then you get your json, copy/paste ubti bucket polic editor, push save.
 
 ## s3 cheat sheet
 
 approx 58 min in, i think you got most of this stuff
